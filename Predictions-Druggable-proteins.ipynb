{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions of Druggable peptides using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_auc_score,f1_score, recall_score, precision_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import RFECV, VarianceThreshold, SelectKBest, chi2\n",
    "from sklearn.feature_selection import SelectFromModel, SelectPercentile, f_classif\n",
    "\n",
    "import seaborn as sns; sns.set() # data visualization library \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataCheckings(df):\n",
    "    # CHECKINGS ***************************\n",
    "    # Check the number of data points in the data set\n",
    "    print(\"\\nData points =\", len(df))\n",
    "    \n",
    "    # Check the number of columns in the data set\n",
    "    print(\"\\nColumns (output + features)=\",len(df.columns))\n",
    "    \n",
    "    # Check the data types\n",
    "    print(\"\\nData types =\", df.dtypes.unique())\n",
    "    \n",
    "    # Dataset statistics\n",
    "    print('\\n')\n",
    "    df.describe()\n",
    "    \n",
    "    # print names of columns\n",
    "    print('Column Names:\\n', df.columns)\n",
    "    \n",
    "    # see if there are categorical data\n",
    "    print(\"\\nCategorical features:\", df.select_dtypes(include=['O']).columns.tolist())\n",
    "    \n",
    "    # Check NA values\n",
    "    # Check any number of columns with NaN\n",
    "    print(\"\\nColumns with NaN: \", df.isnull().any().sum(), ' / ', len(df.columns))\n",
    "\n",
    "    # Check any number of data points with NaN\n",
    "    print(\"\\nNo of data points with NaN:\", df.isnull().any(axis=1).sum(), ' / ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromDataset(sFile, OutVar):\n",
    "    # read details file\n",
    "    print('\\n-> Read dataset', sFile)\n",
    "    df = pd.read_csv(sFile)\n",
    "    #df = feather.read_dataframe(sFile)\n",
    "    \n",
    "    DataCheckings(df)\n",
    "    \n",
    "    # remove duplicates!\n",
    "    df.drop_duplicates(keep=False, inplace=True)\n",
    "    \n",
    "    print('Shape', df.shape)\n",
    "    # print(list(df.columns))\n",
    "\n",
    "    # select X and Y\n",
    "    ds_y = df[OutVar]\n",
    "    ds_X = df.drop(OutVar,axis = 1)\n",
    "    Xdata = ds_X.values # get values of features\n",
    "    Ydata = ds_y.values # get output values\n",
    "\n",
    "    print('Shape X data:', Xdata.shape)\n",
    "    print('Shape Y data:',Ydata.shape)\n",
    "    \n",
    "    # return data for X and Y, feature names as list\n",
    "    return (Xdata, Ydata, list(ds_X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  set_weights(y_data, option='balanced'):\n",
    "    \"\"\"Estimate class weights for umbalanced dataset\n",
    "       If ‘balanced’, class weights will be given by n_samples / (n_classes * np.bincount(y)). \n",
    "       If a dictionary is given, keys are classes and values are corresponding class weights. \n",
    "       If None is given, the class weights will be uniform \"\"\"\n",
    "    cw = class_weight.compute_class_weight(option, np.unique(y_data), y_data)\n",
    "    w = {i:j for i,j in zip(np.unique(y_data), cw)}\n",
    "    return w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define output variables\n",
    "outVar = 'Class'\n",
    "\n",
    "# define list of folds\n",
    "foldType = 3\n",
    "\n",
    "# define a label for output files\n",
    "targetName = 'GS_Outer'\n",
    "\n",
    "seed = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce the pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Read dataset ./datasets/ds.Class_TC_ballanced.csv\n",
      "\n",
      "Data points = 1332\n",
      "\n",
      "Columns (output + features)= 8001\n",
      "\n",
      "Data types = [dtype('float64') dtype('int64')]\n",
      "\n",
      "\n",
      "Column Names:\n",
      " Index(['AAA', 'RAA', 'NAA', 'DAA', 'CAA', 'EAA', 'QAA', 'GAA', 'HAA', 'IAA',\n",
      "       ...\n",
      "       'KVV', 'MVV', 'FVV', 'PVV', 'SVV', 'TVV', 'WVV', 'YVV', 'VVV', 'Class'],\n",
      "      dtype='object', length=8001)\n",
      "\n",
      "Categorical features: []\n",
      "\n",
      "Columns with NaN:  0  /  8001\n",
      "\n",
      "No of data points with NaN: 0  /  1332\n",
      "Shape (1332, 8001)\n",
      "Shape X data: (1332, 8000)\n",
      "Shape Y data: (1332,)\n"
     ]
    }
   ],
   "source": [
    "sFile = './datasets/ds.Class_TC_ballanced.csv'\n",
    "\n",
    "# get data from file\n",
    "Xdata, Ydata, Features = getDataFromDataset(sFile,outVar) # n_sample=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights =  {0: 1.0, 1: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights\n",
    "class_weights = set_weights(Ydata)\n",
    "print(\"Class weights = \", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = StratifiedKFold(n_splits=3,shuffle=True,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 1\n",
      "AUROC= 0.9707410112815518 ACC= 0.9301801801801802 2.053646918137868 mins\n",
      "Fold = 2\n",
      "AUROC= 0.9752049346643942 ACC= 0.9369369369369369 1.0309011777242025 mins\n",
      "Fold = 3\n",
      "AUROC= 0.9783499715932148 ACC= 0.9211711711711712 1.037073588371277 mins\n"
     ]
    }
   ],
   "source": [
    "ifold = 0\n",
    "ACCs  =[]\n",
    "AUROCs=[]\n",
    "models =[]\n",
    "SelectedFeatures =[]\n",
    "\n",
    "for train_index, test_index in outer_cv.split(Xdata, Ydata):\n",
    "    ifold +=1\n",
    "    \n",
    "    print(\"Fold =\",ifold)\n",
    "    start = time.time()\n",
    "    \n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = Xdata[train_index], Xdata[test_index]\n",
    "    y_train, y_test = Ydata[train_index], Ydata[test_index]\n",
    "    \n",
    "    # Standardize dataset\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "    \n",
    "    # Feature selection # FS = SelectFromModel(LinearSVC(), max_features = 400,threshold=-np.inf)\n",
    "    lsvc = LinearSVC(max_iter=50000).fit(X_train, y_train)\n",
    "    model = SelectFromModel(lsvc, prefit=True,max_features = 200,threshold=-np.inf)\n",
    "    X_train = model.transform(X_train)\n",
    "    X_test  = model.transform(X_test)\n",
    "    #print(\"Selected X:\", X_train.shape)\n",
    "\n",
    "    # Selected features\n",
    "    SelFeatures = []\n",
    "    for i in model.get_support(indices=True):\n",
    "        SelFeatures.append(Features[i])\n",
    "    SelectedFeatures.append(SelFeatures)\n",
    "\n",
    "    #scaler.transform(X_test)\n",
    "    clf = SVC(kernel = 'rbf', random_state=seed,gamma='scale',\n",
    "              class_weight=class_weights,probability=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    joblib.dump(clf, 'SVM_model'+str(ifold)+'.pkl', compress = 1)\n",
    "    models.append(clf)\n",
    "    \n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    AUROC = roc_auc_score(y_test, y_pred[:, 1])\n",
    "    AUROCs.append(AUROC)\n",
    "    \n",
    "    ACC = clf.score(X_test,y_test)\n",
    "    ACCs.append(ACC)\n",
    "   \n",
    "    print(\"AUROC=\",AUROC,\"ACC=\",ACC, (time.time() - start)/60,\"mins\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the mean AUROC values for best model and the standard deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.974765305846387 0.0031218610239548486\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(AUROCs),np.std(AUROCs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9294294294294295 0.006458202152434386\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(ACCs),np.std(ACCs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NRA', 'LDA', 'KCA', 'EHA', 'THA', 'VIA', 'YLA', 'IMA', 'CSA', 'VYA', 'KNR', 'TDR', 'WDR', 'TER', 'PQR', 'WMR', 'PPR', 'RSR', 'VSR', 'ETR', 'FWR', 'NYR', 'PVR', 'NAN', 'YNN', 'YIN', 'FMN', 'HFN', 'HDD', 'SDD', 'ACD', 'MED', 'WED', 'TQD', 'SHD', 'WLD', 'VFD', 'WPD', 'SSD', 'LTD', 'PWD', 'EYD', 'VYD', 'WRC', 'TGC', 'IHC', 'WHC', 'VHC', 'WLC', 'GMC', 'GFC', 'GYC', 'EVC', 'GDE', 'SDE', 'SQE', 'IGE', 'NHE', 'NME', 'HME', 'YPE', 'TSE', 'AWE', 'EYE', 'QYE', 'SAQ', 'MRQ', 'INQ', 'PCQ', 'WEQ', 'AQQ', 'RQQ', 'KQQ', 'NGQ', 'CHQ', 'HLQ', 'HKQ', 'DSQ', 'FVQ', 'CDG', 'HGG', 'KGG', 'FGG', 'FPG', 'AWG', 'RVG', 'IVG', 'PAH', 'PDH', 'TQH', 'DGH', 'CGH', 'KHH', 'FWH', 'TYH', 'LVH', 'PAI', 'QNI', 'WEI', 'ESI', 'AYI', 'DRL', 'PDL', 'GPL', 'WYL', 'DVL', 'MVL', 'QRK', 'HNK', 'HCK', 'PHK', 'CIK', 'QLK', 'SMK', 'HTK', 'VYK', 'DVK', 'WRM', 'REM', 'FEM', 'WQM', 'DHM', 'SHM', 'LLM', 'TPM', 'KYM', 'IVM', 'WVM', 'YVM', 'CNF', 'LDF', 'SEF', 'EQF', 'YQF', 'VGF', 'YKF', 'AFF', 'CFF', 'FWF', 'CNP', 'QGP', 'PIP', 'AKP', 'QPP', 'RSP', 'KWP', 'YWP', 'SAS', 'SRS', 'HCS', 'LES', 'IGS', 'WGS', 'DHS', 'GIS', 'SPS', 'WWS', 'WYS', 'TAT', 'DRT', 'GRT', 'INT', 'PET', 'VQT', 'FHT', 'YHT', 'NLT', 'DLT', 'WLT', 'MFT', 'QWT', 'GYT', 'PDW', 'RCW', 'RGW', 'VGW', 'MIW', 'CKW', 'GKW', 'AMW', 'RFW', 'DFW', 'VFW', 'FYW', 'DVW', 'MVW', 'NRY', 'TGY', 'QIY', 'LMY', 'TFY', 'KPY', 'YPY', 'SRV', 'HNV', 'QGV', 'TGV', 'GHV', 'YFV', 'SPV'], ['NRA', 'QRA', 'INA', 'MCA', 'YEA', 'THA', 'CSA', 'VYA', 'KNR', 'WDR', 'TER', 'PQR', 'YGR', 'EHR', 'LIR', 'VSR', 'ERN', 'MDN', 'SDN', 'LHN', 'YIN', 'FFN', 'RSN', 'QSN', 'FWN', 'ACD', 'WCD', 'MED', 'CHD', 'SHD', 'MLD', 'SMD', 'WPD', 'SSD', 'HTD', 'DWD', 'VYD', 'KNC', 'NDC', 'IHC', 'VHC', 'GYC', 'MCE', 'NHE', 'ALE', 'HME', 'LPE', 'AWE', 'EYE', 'QYE', 'GVE', 'FVE', 'SAQ', 'FNQ', 'MDQ', 'PCQ', 'WEQ', 'RQQ', 'NGQ', 'HLQ', 'RMQ', 'DFQ', 'GPQ', 'DSQ', 'YSQ', 'AWQ', 'RVQ', 'QRG', 'HGG', 'TGG', 'KLG', 'NKG', 'FPG', 'SSG', 'RTG', 'PTG', 'IVG', 'CDH', 'FDH', 'PDH', 'TQH', 'KHH', 'FHH', 'IFH', 'NSH', 'WSH', 'FWH', 'WRI', 'NDI', 'EDI', 'FEI', 'WEI', 'WQI', 'MGI', 'PMI', 'AAL', 'EKL', 'IKL', 'FKL', 'GPL', 'ESL', 'DVL', 'MVL', 'VVL', 'GNK', 'HNK', 'HDK', 'HCK', 'EQK', 'DHK', 'QLK', 'EKK', 'SMK', 'FFK', 'QSK', 'EWK', 'AVK', 'WRM', 'WNM', 'REM', 'WQM', 'SHM', 'LLM', 'SMM', 'NFM', 'TSM', 'RWM', 'GYM', 'KYM', 'VYM', 'HVM', 'IVM', 'LDF', 'YQF', 'NGF', 'HGF', 'FWF', 'FAP', 'FNP', 'PEP', 'SQP', 'QGP', 'VHP', 'PLP', 'HKP', 'NPP', 'QPP', 'STP', 'TTP', 'KWP', 'YWP', 'SRS', 'HDS', 'WDS', 'HCS', 'LES', 'DHS', 'SHS', 'PSS', 'SSS', 'LWS', 'LAT', 'DRT', 'GRT', 'IRT', 'INT', 'VQT', 'NLT', 'CLT', 'KKT', 'YTT', 'QWT', 'FYT', 'KCW', 'QGW', 'VGW', 'MIW', 'IKW', 'RFW', 'DFW', 'HVW', 'KVW', 'NRY', 'CHY', 'DMY', 'YPY', 'YAV', 'SRV', 'ENV', 'HNV', 'GEV', 'QGV', 'HGV', 'TGV', 'WHV', 'LLV', 'IMV', 'DSV', 'TSV', 'QYV'], ['NRA', 'KCA', 'YEA', 'AQA', 'DGA', 'HLA', 'VYA', 'ARR', 'TER', 'AQR', 'RHR', 'EHR', 'LIR', 'FFR', 'TFR', 'GSR', 'ERN', 'CNN', 'MDN', 'RHN', 'VHN', 'YIN', 'RSN', 'MVN', 'SCD', 'RGD', 'LGD', 'SHD', 'PKD', 'DFD', 'VFD', 'WPD', 'SSD', 'TSD', 'LTD', 'STD', 'DWD', 'EYD', 'VYD', 'IAC', 'VAC', 'CRC', 'WRC', 'NDC', 'LDC', 'VGC', 'NHC', 'IHC', 'VHC', 'EMC', 'GFC', 'LPC', 'HTC', 'GYC', 'EVC', 'GDE', 'KDE', 'FHE', 'RLE', 'HME', 'TSE', 'KWE', 'EYE', 'QYE', 'GVE', 'SAQ', 'YAQ', 'INQ', 'NEQ', 'WEQ', 'RQQ', 'NGQ', 'EHQ', 'HLQ', 'VKQ', 'HFQ', 'GPQ', 'DSQ', 'QSQ', 'DWQ', 'RVQ', 'FVQ', 'NAG', 'CDG', 'HGG', 'GHG', 'HHG', 'KLG', 'NKG', 'RTG', 'RVG', 'YAH', 'CNH', 'FDH', 'PDH', 'DGH', 'CGH', 'KHH', 'VSH', 'CYH', 'WRI', 'WEI', 'NII', 'PMI', 'ESI', 'FTI', 'RDL', 'QDL', 'EKL', 'GPL', 'VWL', 'DVL', 'MVL', 'VVL', 'HNK', 'KDK', 'MDK', 'HCK', 'QLK', 'WKK', 'SMK', 'TYK', 'WRM', 'WNM', 'REM', 'IEM', 'FEM', 'WQM', 'SHM', 'LLM', 'TSM', 'IVM', 'WVM', 'YVM', 'LDF', 'TCF', 'CEF', 'SEF', 'VEF', 'YQF', 'AIF', 'YKF', 'MMF', 'CFF', 'GWF', 'FWF', 'TYF', 'QNP', 'FNP', 'QGP', 'EIP', 'PLP', 'PPP', 'TTP', 'KWP', 'YWP', 'RVP', 'SRS', 'CDS', 'HCS', 'LES', 'DHS', 'SHS', 'TAT', 'DRT', 'GRT', 'INT', 'FNT', 'YNT', 'VQT', 'NLT', 'KKT', 'MFT', 'KTT', 'QWT', 'PWT', 'RCW', 'PCW', 'MIW', 'WLW', 'GKW', 'IKW', 'RFW', 'DFW', 'RWW', 'QYW', 'NRY', 'YRY', 'TGY', 'DMY', 'LMY', 'YPY', 'YAV', 'SRV', 'ENV', 'HNV', 'HGV', 'TGV', 'VHV', 'LLV']]\n"
     ]
    }
   ],
   "source": [
    "# all the selected features for the 3 folds\n",
    "print(SelectedFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AVK', 'NKG', 'CHY', 'EKL', 'RWM', 'HGV', 'NFM', 'QSN', 'WHV', 'CHD', 'WSH', 'FHH', 'HVM', 'NDC', 'NSH', 'SSS', 'RSN', 'GYM', 'MLD', 'YAV', 'FNP', 'LWS', 'FEI', 'NPP', 'PSS', 'HGF', 'LIR', 'NDI', 'TTP', 'ERN', 'DHK', 'ESL', 'HDS', 'GNK', 'FDH', 'HKP', 'SMM', 'FYT', 'FNQ', 'HTD', 'CDH', 'YSQ', 'YGR', 'ALE', 'HDK', 'CLT', 'LPE', 'MCA', 'IKL', 'FFN', 'SSG', 'EWK', 'LHN', 'WNM', 'SQP', 'GEV', 'AWQ', 'DMY', 'FKL', 'INA', 'KLG', 'SHS', 'QGW', 'DFQ', 'YTT', 'QYV', 'DSV', 'IRT', 'FWN', 'FVE', 'RTG', 'GPQ', 'MGI', 'WQI', 'VYM', 'EQK', 'STP', 'KCW', 'LLV', 'VHP', 'MCE', 'HVW', 'NGF', 'TSM', 'SMD', 'WDS', 'LAT', 'WRI', 'SDN', 'FFK', 'QRA', 'TSV', 'GVE', 'RMQ', 'EHR', 'QSK', 'KKT', 'FAP', 'RVQ', 'VVL', 'WCD', 'TGG', 'ENV', 'QRG', 'IFH', 'YEA', 'DWD', 'EKK', 'MDN', 'PTG', 'PLP', 'MDQ', 'PEP', 'KVW', 'IKW', 'IMV', 'KNC', 'EDI', 'AAL', 'PMI']\n"
     ]
    }
   ],
   "source": [
    "# differences of selected descriptors\n",
    "print(list(set(SelectedFeatures[1])-set(SelectedFeatures[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PQR', 'AVK', 'THA', 'CHY', 'RWM', 'NFM', 'IVG', 'QSN', 'WHV', 'CHD', 'WSH', 'FHH', 'HVM', 'NSH', 'QPP', 'FPG', 'SSS', 'GYM', 'MLD', 'LWS', 'FEI', 'NPP', 'PSS', 'HGF', 'NDI', 'WDR', 'DHK', 'ESL', 'HDS', 'GNK', 'VSR', 'HKP', 'SMM', 'FYT', 'FNQ', 'HTD', 'CDH', 'YSQ', 'YGR', 'ALE', 'HDK', 'TQH', 'CLT', 'KNR', 'LPE', 'MCA', 'IKL', 'FFN', 'SSG', 'EWK', 'LHN', 'SQP', 'VGW', 'AWQ', 'GEV', 'FKL', 'INA', 'QGW', 'DFQ', 'YTT', 'QYV', 'DSV', 'IRT', 'FWN', 'PCQ', 'ACD', 'FVE', 'MGI', 'WQI', 'VYM', 'EQK', 'STP', 'KCW', 'QGV', 'VHP', 'MCE', 'HVW', 'NGF', 'SMD', 'WDS', 'LAT', 'SDN', 'FFK', 'QRA', 'KYM', 'TSV', 'RMQ', 'QSK', 'FAP', 'FWH', 'WCD', 'TGG', 'CSA', 'QRG', 'IFH', 'EKK', 'AWE', 'PTG', 'MDQ', 'PEP', 'KVW', 'IMV', 'KNC', 'EDI', 'AAL', 'NHE', 'MED']\n"
     ]
    }
   ],
   "source": [
    "# differences of selected descriptors\n",
    "print(list(set(SelectedFeatures[1])-set(SelectedFeatures[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions with the best model\n",
    "\n",
    "We choose model 2 as the best due to the maximum ACC value (AUROC= 0.9752, ACC= 0.937)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NRA', 'QRA', 'INA', 'MCA', 'YEA', 'THA', 'CSA', 'VYA', 'KNR', 'WDR', 'TER', 'PQR', 'YGR', 'EHR', 'LIR', 'VSR', 'ERN', 'MDN', 'SDN', 'LHN', 'YIN', 'FFN', 'RSN', 'QSN', 'FWN', 'ACD', 'WCD', 'MED', 'CHD', 'SHD', 'MLD', 'SMD', 'WPD', 'SSD', 'HTD', 'DWD', 'VYD', 'KNC', 'NDC', 'IHC', 'VHC', 'GYC', 'MCE', 'NHE', 'ALE', 'HME', 'LPE', 'AWE', 'EYE', 'QYE', 'GVE', 'FVE', 'SAQ', 'FNQ', 'MDQ', 'PCQ', 'WEQ', 'RQQ', 'NGQ', 'HLQ', 'RMQ', 'DFQ', 'GPQ', 'DSQ', 'YSQ', 'AWQ', 'RVQ', 'QRG', 'HGG', 'TGG', 'KLG', 'NKG', 'FPG', 'SSG', 'RTG', 'PTG', 'IVG', 'CDH', 'FDH', 'PDH', 'TQH', 'KHH', 'FHH', 'IFH', 'NSH', 'WSH', 'FWH', 'WRI', 'NDI', 'EDI', 'FEI', 'WEI', 'WQI', 'MGI', 'PMI', 'AAL', 'EKL', 'IKL', 'FKL', 'GPL', 'ESL', 'DVL', 'MVL', 'VVL', 'GNK', 'HNK', 'HDK', 'HCK', 'EQK', 'DHK', 'QLK', 'EKK', 'SMK', 'FFK', 'QSK', 'EWK', 'AVK', 'WRM', 'WNM', 'REM', 'WQM', 'SHM', 'LLM', 'SMM', 'NFM', 'TSM', 'RWM', 'GYM', 'KYM', 'VYM', 'HVM', 'IVM', 'LDF', 'YQF', 'NGF', 'HGF', 'FWF', 'FAP', 'FNP', 'PEP', 'SQP', 'QGP', 'VHP', 'PLP', 'HKP', 'NPP', 'QPP', 'STP', 'TTP', 'KWP', 'YWP', 'SRS', 'HDS', 'WDS', 'HCS', 'LES', 'DHS', 'SHS', 'PSS', 'SSS', 'LWS', 'LAT', 'DRT', 'GRT', 'IRT', 'INT', 'VQT', 'NLT', 'CLT', 'KKT', 'YTT', 'QWT', 'FYT', 'KCW', 'QGW', 'VGW', 'MIW', 'IKW', 'RFW', 'DFW', 'HVW', 'KVW', 'NRY', 'CHY', 'DMY', 'YPY', 'YAV', 'SRV', 'ENV', 'HNV', 'GEV', 'QGV', 'HGV', 'TGV', 'WHV', 'LLV', 'IMV', 'DSV', 'TSV', 'QYV']\n"
     ]
    }
   ],
   "source": [
    "# the selected features for model 2\n",
    "print(SelectedFeatures[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the prediction datasets (the same format as the dataset: 8000 TC features + Class=-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Read dataset ./datasets/ds.Screening_1_TC.csv\n",
      "\n",
      "Data points = 2359\n",
      "\n",
      "Columns (output + features)= 8001\n",
      "\n",
      "Data types = [dtype('float64') dtype('int64')]\n",
      "\n",
      "\n",
      "Column Names:\n",
      " Index(['AAA', 'RAA', 'NAA', 'DAA', 'CAA', 'EAA', 'QAA', 'GAA', 'HAA', 'IAA',\n",
      "       ...\n",
      "       'KVV', 'MVV', 'FVV', 'PVV', 'SVV', 'TVV', 'WVV', 'YVV', 'VVV', 'Class'],\n",
      "      dtype='object', length=8001)\n",
      "\n",
      "Categorical features: []\n",
      "\n",
      "Columns with NaN:  0  /  8001\n",
      "\n",
      "No of data points with NaN: 0  /  2359\n",
      "Shape (2353, 8001)\n",
      "Shape X data: (2353, 8000)\n",
      "Shape Y data: (2353,)\n",
      "\n",
      "-> Read dataset ./datasets/ds.Screening_2_TC.csv\n",
      "\n",
      "Data points = 231\n",
      "\n",
      "Columns (output + features)= 8001\n",
      "\n",
      "Data types = [dtype('float64') dtype('int64')]\n",
      "\n",
      "\n",
      "Column Names:\n",
      " Index(['AAA', 'RAA', 'NAA', 'DAA', 'CAA', 'EAA', 'QAA', 'GAA', 'HAA', 'IAA',\n",
      "       ...\n",
      "       'KVV', 'MVV', 'FVV', 'PVV', 'SVV', 'TVV', 'WVV', 'YVV', 'VVV', 'Class'],\n",
      "      dtype='object', length=8001)\n",
      "\n",
      "Categorical features: []\n",
      "\n",
      "Columns with NaN:  0  /  8001\n",
      "\n",
      "No of data points with NaN: 0  /  231\n",
      "Shape (231, 8001)\n",
      "Shape X data: (231, 8000)\n",
      "Shape Y data: (231,)\n",
      "\n",
      "-> Read dataset ./datasets/ds.Screening_3_TC.csv\n",
      "\n",
      "Data points = 1369\n",
      "\n",
      "Columns (output + features)= 8001\n",
      "\n",
      "Data types = [dtype('float64') dtype('int64')]\n",
      "\n",
      "\n",
      "Column Names:\n",
      " Index(['AAA', 'RAA', 'NAA', 'DAA', 'CAA', 'EAA', 'QAA', 'GAA', 'HAA', 'IAA',\n",
      "       ...\n",
      "       'KVV', 'MVV', 'FVV', 'PVV', 'SVV', 'TVV', 'WVV', 'YVV', 'VVV', 'Class'],\n",
      "      dtype='object', length=8001)\n",
      "\n",
      "Categorical features: []\n",
      "\n",
      "Columns with NaN:  0  /  8001\n",
      "\n",
      "No of data points with NaN: 0  /  1369\n",
      "Shape (1365, 8001)\n",
      "Shape X data: (1365, 8000)\n",
      "Shape Y data: (1365,)\n",
      "(231, 8000) (1365, 8000)\n"
     ]
    }
   ],
   "source": [
    "# get data from files and check the files\n",
    "sFile1 = './datasets/ds.Screening_1_TC.csv'\n",
    "Xdata1, Ydata1, Features1 = getDataFromDataset(sFile1,outVar) \n",
    "\n",
    "sFile2 = './datasets/ds.Screening_2_TC.csv'\n",
    "Xdata2, Ydata2, Features2 = getDataFromDataset(sFile2,outVar) \n",
    "\n",
    "sFile3 = './datasets/ds.Screening_3_TC.csv'\n",
    "Xdata3, Ydata3, Features3 = getDataFromDataset(sFile3,outVar)\n",
    "\n",
    "print(Xdata2.shape,Xdata3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use only the second split / model - scale the prediction datasets, select only the features of model 2, predict the class and predict the probability of that class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 2\n",
      "Xdata1 sel= (2353, 200)\n",
      "Xdata2 sel= (231, 200)\n",
      "Xdata3 sel= (1365, 200)\n",
      "Time 1.0715562502543132 mins\n"
     ]
    }
   ],
   "source": [
    "ifold = 0\n",
    "\n",
    "for train_index, test_index in outer_cv.split(Xdata, Ydata):\n",
    "    ifold +=1\n",
    "    \n",
    "    if ifold ==2: # only model 2\n",
    "        print(\"Fold =\",ifold)\n",
    "        start = time.time()\n",
    "\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = Xdata[train_index], Xdata[test_index]\n",
    "        y_train, y_test = Ydata[train_index], Ydata[test_index]\n",
    "\n",
    "        # Standardize dataset\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test  = scaler.transform(X_test)\n",
    "        # scale prediction set\n",
    "        Xdata1  = scaler.transform(Xdata1)\n",
    "        Xdata2  = scaler.transform(Xdata2)\n",
    "        Xdata3  = scaler.transform(Xdata3)\n",
    "        \n",
    "        # Feature selection # FS = SelectFromModel(LinearSVC(), max_features = 400,threshold=-np.inf)\n",
    "        lsvc = LinearSVC(max_iter=50000).fit(X_train, y_train)\n",
    "        model = SelectFromModel(lsvc, prefit=True,max_features = 200,threshold=-np.inf)\n",
    "        X_train = model.transform(X_train)\n",
    "        X_test  = model.transform(X_test)\n",
    "    \n",
    "        # Selected features\n",
    "        SelFeatures = []\n",
    "        for i in model.get_support(indices=True):\n",
    "            SelFeatures.append(Features[i])\n",
    "        \n",
    "        # apply selected features to prediction set\n",
    "        Xdata1 = Xdata1[:, model.get_support()]\n",
    "        print(\"Xdata1 sel=\", Xdata1.shape)\n",
    "        Xdata2 = Xdata2[:, model.get_support()]\n",
    "        print(\"Xdata2 sel=\", Xdata2.shape)\n",
    "        Xdata3 = Xdata3[:, model.get_support()]\n",
    "        print(\"Xdata3 sel=\", Xdata3.shape)\n",
    "\n",
    "        # we dont need to calculate again, but load the model from the disk!\n",
    "        #clf = SVC(kernel = 'rbf', random_state=seed,gamma='scale',\n",
    "        #          class_weight=class_weights,probability=True)\n",
    "        #clf.fit(X_train, y_train)\n",
    "\n",
    "        # load the saved model from disk\n",
    "        clf = joblib.load('SVM_model'+str(ifold)+'.pkl')\n",
    "        #joblib.dump(clf, 'SVM_model'+str(ifold)+'.pkl', compress = 1)\n",
    "        \n",
    "        # predictions with the model\n",
    "        Ydata1 = clf.predict(Xdata1)\n",
    "        Ydata2 = clf.predict(Xdata2)\n",
    "        Ydata3 = clf.predict(Xdata3)\n",
    "        \n",
    "        # add probabilities (n_samples X n_classes; class 0, class 1)\n",
    "        Ydata1prob = clf.predict_proba(Xdata1)\n",
    "        Ydata2prob = clf.predict_proba(Xdata2)\n",
    "        Ydata3prob = clf.predict_proba(Xdata3)\n",
    "        \n",
    "        # save predictions for list 1\n",
    "        dff1 = pd.DataFrame(Xdata1,columns=SelFeatures)\n",
    "        dff1['Class'] = Ydata1\n",
    "        dff1['Prob0'] = Ydata1prob[:,0]\n",
    "        dff1['Prob1'] = Ydata1prob[:,1]\n",
    "        # merge with protein information from other file\n",
    "        result = pd.concat([dff1, pd.read_csv('./PREDICTIONS/TC_seqs.Screening_1_Cancer_Driver_Genes.csv')], axis=1)\n",
    "        # creat new order of columns in final results\n",
    "        newHeader=['Class','Prob1','Prob0','V1','V2']\n",
    "        result = result[newHeader]\n",
    "        result = result.sort_values(by=['Prob1'], ascending=False)\n",
    "        result.to_csv(sFile1[:-4]+'_predictions.csv', index=True)\n",
    "\n",
    "        # save predictions for list 2\n",
    "        dff2 = pd.DataFrame(Xdata2,columns=SelFeatures)\n",
    "        dff2['Class'] = Ydata2\n",
    "        dff2['Prob0'] = Ydata2prob[:,0]\n",
    "        dff2['Prob1'] = Ydata2prob[:,1]\n",
    "        # merge with protein information from other file\n",
    "        result = pd.concat([dff2, pd.read_csv('./PREDICTIONS/TC_seqs.Screening_2_OncoOmics_Genes.csv')], axis=1)\n",
    "        # creat new order of columns in final results\n",
    "        newHeader=['Class','Prob1','Prob0','V1','V2']\n",
    "        result = result[newHeader]\n",
    "        result = result.sort_values(by=['Prob1'], ascending=False)\n",
    "        result.to_csv(sFile2[:-4]+'_predictions.csv', index=True)\n",
    "        \n",
    "        # save predictions for list 3\n",
    "        dff3 = pd.DataFrame(Xdata3,columns=SelFeatures)\n",
    "        dff3['Class'] = Ydata3\n",
    "        dff3['Prob0'] = Ydata3prob[:,0]\n",
    "        dff3['Prob1'] = Ydata3prob[:,1]\n",
    "        # merge with protein information from other file\n",
    "        result = pd.concat([dff3, pd.read_csv('./PREDICTIONS/TC_seqs.Screening_3_RBPs.csv')], axis=1)\n",
    "        # creat new order of columns in final results\n",
    "        newHeader=['Class','Prob1','Prob0','V1','V2']\n",
    "        result = result[newHeader]\n",
    "        result = result.sort_values(by=['Prob1'], ascending=False)\n",
    "        result.to_csv(sFile3[:-4]+'_predictions.csv', index=True)\n",
    "\n",
    "        print(\"Time\",(time.time() - start)/60,\"mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Chekc the results:\n",
      "./datasets/ds.Screening_1_TC_predictions.csv\n",
      "./datasets/ds.Screening_2_TC_predictions.csv\n",
      "./datasets/ds.Screening_3_TC_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"==> Chekc the results:\")\n",
    "print(sFile1[:-4]+'_predictions.csv')\n",
    "print(sFile2[:-4]+'_predictions.csv')\n",
    "print(sFile3[:-4]+'_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hf with ML!@muntisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
